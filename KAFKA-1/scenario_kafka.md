# Сценарии лабораторных работ по Apache Kafka

## Лабораторная работа 1: Основы Kafka - Producer и Consumer

**Цель:** Познакомиться с базовыми концепциями Kafka, научиться создавать producer и consumer.

**Задачи:**
1. Установить и запустить Kafka локально
2. Создать топик `user-events`
3. Написать простой producer на Python, отправляющий сообщения в топик
4. Написать consumer, читающий сообщения из топика
5. Протестировать отправку и получение 100 сообщений

**Результат:** Работающая связка producer-consumer с выводом сообщений в консоль.

---

## Лабораторная работа 2: Partitions и Consumer Groups

**Цель:** Изучить механизм партиционирования и распределения нагрузки через consumer groups.

**Задачи:**
1. Создать топик с 3 партициями
2. Реализовать producer с кастомным partitioner (по user_id)
3. Запустить 3 consumer в одной группе
4. Наблюдать распределение сообщений между consumers
5. Добавить/удалить consumer и увидеть rebalancing

**Результат:** Понимание работы партиций и балансировки нагрузки.

---

## Лабораторная работа 3: Обработка заказов в e-commerce

**Цель:** Реализовать реальный сценарий обработки заказов.

**Задачи:**
1. Создать топики: `orders`, `payments`, `notifications`
2. Producer отправляет заказы в `orders`
3. Payment Service читает из `orders`, обрабатывает и пишет в `payments`
4. Notification Service читает из `payments` и отправляет уведомления
5. Реализовать обработку ошибок и retry логику

**Результат:** Микросервисная архитектура с Kafka как message broker.

---

## Лабораторная работа 4: Kafka Streams - Агрегация данных

**Цель:** Освоить Kafka Streams для обработки потоков данных.

**Задачи:**
1. Создать топик с событиями кликов пользователей
2. Использовать Kafka Streams для подсчета кликов по категориям за окно времени (5 минут)
3. Результаты записывать в отдельный топик
4. Визуализировать статистику в реальном времени

**Результат:** Stream processing приложение с агрегацией.

---

## Лабораторная работа 5: Мониторинг и производительность

**Цель:** Научиться мониторить Kafka и оптимизировать производительность.

**Задачи:**
1. Настроить JMX метрики для Kafka
2. Подключить Prometheus и Grafana
3. Провести нагрузочное тестирование (throughput, latency)
4. Оптимизировать параметры: batch.size, linger.ms, compression
5. Сравнить результаты до и после оптимизации

**Результат:** Dashboard с метриками и отчет по оптимизации.

---

## Лабораторная работа 6: Exactly-Once Semantics

**Цель:** Реализовать гарантии exactly-once доставки.

**Задачи:**
1. Настроить transactional producer
2. Реализовать idempotent producer
3. Создать consumer с ручным управлением offset
4. Симулировать сбои и проверить отсутствие дубликатов
5. Сравнить at-least-once vs exactly-once

**Результат:** Система с гарантией exactly-once обработки.

---

## Лабораторная работа 7: Schema Registry и Avro

**Цель:** Использовать Schema Registry для управления схемами данных.

**Задачи:**
1. Установить Confluent Schema Registry
2. Определить Avro схему для сообщений
3. Реализовать producer с сериализацией в Avro
4. Реализовать consumer с десериализацией
5. Обновить схему (добавить поле) и проверить совместимость

**Результат:** Типизированные сообщения с версионированием схем.

---

## Лабораторная работа 8: Kafka Connect

**Цель:** Интегрировать Kafka с внешними системами через Kafka Connect.

**Задачи:**
1. Настроить Kafka Connect
2. Создать Source Connector для PostgreSQL (CDC)
3. Создать Sink Connector для Elasticsearch
4. Настроить трансформации данных
5. Проверить синхронизацию данных

**Результат:** Pipeline: PostgreSQL → Kafka → Elasticsearch.

---

## Лабораторная работа 9: Multi-datacenter репликация

**Цель:** Настроить репликацию между кластерами Kafka.

**Задачи:**
1. Развернуть два Kafka кластера
2. Настроить MirrorMaker 2
3. Реализовать active-passive репликацию
4. Протестировать failover
5. Измерить lag репликации

**Результат:** Отказоустойчивая архитектура с репликацией.

---

## Лабораторная работа 10: Real-time Analytics Dashboard

**Цель:** Создать полноценное приложение для аналитики в реальном времени.

**Задачи:**
1. Producer генерирует события IoT устройств (температура, влажность)
2. Kafka Streams агрегирует данные по временным окнам
3. Обнаруживать аномалии (выход за пороги)
4. Отправлять алерты в отдельный топик
5. Web-интерфейс отображает данные через WebSocket

**Результат:** End-to-end решение для мониторинга IoT.

---

## Лабораторная работа 11: Event Sourcing и CQRS

**Цель:** Реализовать паттерны Event Sourcing и CQRS с использованием Kafka.

**Задачи:**
1. Создать систему управления банковскими счетами
2. Все изменения сохранять как события в Kafka (AccountCreated, MoneyDeposited, MoneyWithdrawn)
3. Реализовать Command Service для записи событий
4. Реализовать Query Service с материализованным представлением (read model)
5. Восстановить состояние счета из истории событий

**Результат:** Система с полной историей изменений и разделением команд/запросов.

---

## Лабораторная работа 12: Dead Letter Queue и Error Handling

**Цель:** Научиться обрабатывать ошибки и проблемные сообщения.

**Задачи:**
1. Создать топики: `main-topic`, `retry-topic`, `dlq-topic`
2. Consumer обрабатывает сообщения с возможностью ошибок
3. При ошибке отправлять в retry-topic с экспоненциальной задержкой
4. После 3 попыток отправлять в DLQ
5. Реализовать admin-панель для ручной обработки DLQ

**Результат:** Надежная система обработки с управлением ошибками.

---

## Лабораторная работа 13: Kafka в микросервисах - Saga Pattern

**Цель:** Реализовать распределенные транзакции через Saga паттерн.

**Задачи:**
1. Создать сервисы: Order, Payment, Inventory, Delivery
2. Реализовать Choreography Saga через Kafka события
3. Обработать успешный сценарий заказа
4. Реализовать компенсирующие транзакции при ошибках
5. Визуализировать flow событий

**Результат:** Распределенная транзакция с автоматическим rollback.

---

## Лабораторная работа 14: Stream Joins и Enrichment

**Цель:** Освоить соединение потоков данных в Kafka Streams.

**Задачи:**
1. Создать топики: `user-clicks`, `user-profiles`, `products`
2. Реализовать stream-table join для обогащения кликов профилями
3. Реализовать stream-stream join для корреляции событий
4. Добавить windowed join для временных окон
5. Результаты записать в enriched-events топик

**Результат:** Обогащенные события с данными из нескольких источников.

---

## Лабораторная работа 15: Kafka Security - Authentication & Authorization

**Цель:** Настроить безопасность Kafka кластера.

**Задачи:**
1. Настроить SSL/TLS шифрование
2. Включить SASL аутентификацию (SCRAM-SHA-256)
3. Настроить ACL для топиков и consumer groups
4. Создать разных пользователей с разными правами
5. Протестировать доступ и ограничения

**Результат:** Защищенный Kafka кластер с контролем доступа.

---

## Лабораторная работа 16: Change Data Capture (CDC) с Debezium

**Цель:** Реализовать CDC для синхронизации баз данных.

**Задачи:**
1. Настроить MySQL с binlog
2. Установить Debezium MySQL Connector
3. Захватывать изменения таблиц в Kafka
4. Реализовать consumer для репликации в другую БД
5. Протестировать INSERT, UPDATE, DELETE операции

**Результат:** Real-time репликация данных через CDC.

---

## Лабораторная работа 17: Kafka для Machine Learning Pipeline

**Цель:** Использовать Kafka в ML pipeline для обучения и инференса.

**Задачи:**
1. Producer отправляет данные для обучения модели
2. Training Service читает данные и обучает модель
3. Публиковать метрики обучения в отдельный топик
4. Inference Service использует модель для предсказаний в реальном времени
5. Мониторить drift модели через Kafka Streams

**Результат:** ML pipeline с online learning и inference.

---

## Лабораторная работа 18: Log Aggregation System

**Цель:** Создать централизованную систему сбора логов.

**Задачи:**
1. Настроить Filebeat для отправки логов в Kafka
2. Создать топики по уровням логов (info, warning, error)
3. Kafka Streams для фильтрации и парсинга логов
4. Обнаруживать паттерны ошибок и аномалии
5. Интеграция с Elasticsearch и Kibana для визуализации

**Результат:** ELK stack с Kafka как транспортный слой.

---

## Лабораторная работа 19: Rate Limiting и Throttling

**Цель:** Реализовать контроль нагрузки через Kafka.

**Задачи:**
1. Producer генерирует события с разной интенсивностью
2. Реализовать rate limiter на основе Kafka Streams
3. Использовать sliding window для подсчета запросов
4. Отклонять или задерживать превышающие лимит запросы
5. Динамически изменять лимиты через конфигурационный топик

**Результат:** Система защиты от перегрузки с гибкими лимитами.

---

## Лабораторная работа 20: Kafka Testing - Integration & Contract Tests

**Цель:** Научиться тестировать Kafka приложения.

**Задачи:**
1. Настроить EmbeddedKafka для unit-тестов
2. Написать integration тесты с Testcontainers
3. Реализовать contract tests для producer/consumer
4. Тестировать обработку ошибок и retry логику
5. Измерить code coverage

**Результат:** Полностью протестированное Kafka приложение.

---

## Лабораторная работа 21: Kafka для Gaming - Matchmaking System

**Цель:** Создать систему подбора игроков в реальном времени.

**Задачи:**
1. Producer отправляет запросы игроков на поиск матча
2. Matchmaking Service группирует игроков по рейтингу и региону
3. Использовать Kafka Streams для агрегации очереди
4. Создавать матчи при достижении нужного количества игроков
5. Отправлять уведомления о найденном матче

**Результат:** Real-time matchmaking система для онлайн игр.

---

## Лабораторная работа 22: Kafka Tiered Storage

**Цель:** Настроить многоуровневое хранилище для оптимизации затрат.

**Задачи:**
1. Настроить Kafka с Tiered Storage (S3/MinIO)
2. Конфигурировать политики перемещения данных
3. Тестировать чтение из hot и cold storage
4. Измерить разницу в производительности
5. Рассчитать экономию на хранении

**Результат:** Оптимизированное хранилище с балансом цена/производительность.

---

## Лабораторная работа 23: Real-time Recommendation Engine

**Цель:** Построить систему рекомендаций в реальном времени.

**Задачи:**
1. Собирать события пользователей (просмотры, клики, покупки)
2. Kafka Streams вычисляет похожесть товаров
3. Обновлять рекомендации в KTable
4. API сервис читает рекомендации из Kafka
5. Персонализировать рекомендации на основе истории

**Результат:** Динамическая система рекомендаций с низкой латентностью.

---

## Лабораторная работа 24: Kafka для Blockchain Events

**Цель:** Интегрировать Kafka с блокчейн событиями.

**Задачи:**
1. Слушать события смарт-контрактов (Web3)
2. Публиковать транзакции и события в Kafka
3. Обрабатывать подтверждения блоков
4. Агрегировать статистику по адресам и контрактам
5. Создать dashboard для мониторинга блокчейн активности

**Результат:** Real-time аналитика блокчейн данных через Kafka.

---

## Лабораторная работа 25: Multi-tenancy в Kafka

**Цель:** Реализовать изоляцию данных для разных клиентов.

**Задачи:**
1. Создать топики с префиксами по tenant_id
2. Настроить квоты для каждого tenant
3. Реализовать роутинг сообщений по tenant
4. Изолировать consumer groups
5. Мониторить использование ресурсов по tenant

**Результат:** SaaS платформа с изоляцией данных клиентов.

---

## Лабораторная работа 26: Dating App - Сайт знакомств с Kafka

**Цель:** Создать полнофункциональную систему знакомств с real-time взаимодействием через Kafka.

**Задачи:**

1. **User Activity Stream**
   - Топик `user-online-status` для отслеживания онлайн/оффлайн статуса
   - Топик `user-profile-updates` для изменений профиля
   - Producer отправляет события при входе/выходе пользователя

2. **Matching Engine**
   - Топик `match-requests` для запросов на поиск пары
   - Kafka Streams фильтрует по критериям (возраст, город, интересы)
   - Вычисление compatibility score на основе профилей
   - Топик `potential-matches` с результатами подбора

3. **Real-time Messaging**
   - Топик `chat-messages` партиционированный по chat_id
   - Consumer group для доставки сообщений
   - Топик `message-read-receipts` для статусов прочтения
   - WebSocket сервис читает из Kafka и отправляет клиентам

4. **Likes & Swipes System**
   - Топик `user-swipes` (like/dislike события)
   - Kafka Streams обнаруживает взаимные лайки (match)
   - Топик `mutual-matches` для уведомлений о совпадениях
   - Отправка push-уведомлений при match

5. **Analytics & Recommendations**
   - Kafka Streams агрегирует статистику активности
   - Вычисление популярности профилей
   - ML модель для улучшения алгоритма подбора
   - Топик `user-recommendations` с персональными рекомендациями

6. **Moderation & Safety**
   - Топик `reported-users` для жалоб
   - Автоматическая фильтрация неприемлемого контента
   - Топик `blocked-users` для блокировок
   - Dead Letter Queue для подозрительной активности

**Архитектура топиков:**
```
user-online-status (key: user_id)
user-profile-updates (key: user_id)
match-requests (key: user_id)
potential-matches (key: user_id)
chat-messages (key: chat_id)
message-read-receipts (key: message_id)
user-swipes (key: user_id)
mutual-matches (key: match_id)
user-recommendations (key: user_id)
reported-users (key: report_id)
blocked-users (key: user_id)
```

**Микросервисы:**
- Profile Service - управление профилями
- Matching Service - алгоритм подбора пар
- Chat Service - обмен сообщениями
- Notification Service - уведомления
- Analytics Service - статистика и аналитика
- Moderation Service - модерация контента

**Результат:** Полноценная платформа знакомств с real-time функциями, умным подбором пар и безопасностью.
